<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">
    code {
      white-space: pre;
    }
  </style>
  <link rel="stylesheet" href="description.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body>
  <div class="row">
    <div class="col">
      <h3 id="about">About</h3>
    </div>
    <div class="col-md-6 description">
      <h4 id="data">Data</h4>
      <p>Here we show functional connectivity networks during an out-loud reading task. Electrocorticography (ECOG) signals were recorded while subjects read the words of a famous speech or nursery rhyme out loud as they scrolled across a computer screen.</p>
      <p>Trials (
        <em>Speech</em>) were defined as the time period 500 ms before until 500 ms after the onset of speech, and they were compared to baseline data when the subject was not speaking (
        <em>Silence</em>).</p>
      <p>Note that the subjects were not speaking before time 0 (speech onset) during the trials, although this time period likely includes neural activity related to reading and speech preparation. The trials were analyzed using a 200 ms sliding window.</p>
      <h4 id="types-of-networks">Types of Networks</h4>
      <p>Five network types are available:</p>
      <ul>
        <li>
          <p>
            <strong>Coherence difference</strong>.
            <span class="math inline">\(Coh(Speech) - Coh(Silence)\)</span>
          </p>
        </li>
        <li>
          <p>
            <strong>Weighted coherence</strong>.
            <span class="math inline">\(\hat{z}_{coh} = \frac{atanh(Coh(Speech)) - \frac{1}{2LP-2} - atanh(Coh(Silence) - \frac{1}{2KP-2}}{\sqrt{var_{jk}(\hat{z}_{coh})}}\)</span>
          </p>
        </li>
        <li>
          <p>
            <strong>Two-sided binary coherence</strong>. Two-sided test for
            <span class="math inline">\(H_0: \hat{z}_{coh}=0\)</span>, corrected for multiple comparisons using a false discovery rate criterion of 5%</p>
        </li>
        <li>
          <p>
            <strong>Weighted correlation</strong>.
            <span class="math inline">\(\hat{z}_{corr} = \frac{atanh(Corr(Speech)) - atanh(Corr(Silence))}{\sqrt{(var_{jk}(\hat{z}_{corr}))}}\)</span>
          </p>
        </li>
        <li>
          <p>
            <strong>Two-sided binary correlation</strong>. Two-sided test for
            <span class="math inline">\(H_0: \hat{z}_{corr}=0\)</span>, corrected for multiple comparisons using a false discovery rate criterion of 5%</p>
        </li>
      </ul>
      <p>where
        <span class="math inline">\(Coh(.)\)</span> is coherence between two edges at a particular time and frequency,
        <span class="math inline">\(Corr(.)\)</span> is correlation between two edges at a particular time,
        <span class="math inline">\(atanh(.)\)</span> is the Fisher transform,
        <span class="math inline">\(var_{jk}(.)\)</span> is the variance estimated using a two-sample jackknife-procedure,
        <span class="math inline">\(L\)</span> is the number of speech trials,
        <span class="math inline">\(K\)</span> is the number of silence intervals, and
        <span class="math inline">\(P\)</span> is the number of tapers used in the multitaper estimate of the coherence.</p>
      <p>Under the null hypothesis of no coherence (or correlation) between the electrodes,
        <span class="math inline">\(\hat{z}_{coh}\)</span> (
        <span class="math inline">\(\hat{z}_{corr}\)</span>) will be approximately distributed as a standard normal.</p>
      <p>All frequency-domain statistics have a frequency resolution of +/- 5 Hz.</p>
    </div>
    <div class="col-md-6 description">
      <h4 id="visualization">Visualization</h4>
      <p>The selected network type is shown for a particular time and frequency, which can be chosen using the sliders on the right or by hovering over the spectrograms/coherograms/correlograms below.</p>
      <p>Below the network view are shown several detail plots for a selected edge (a different edge can be selected by clicking the edge or by clicking on two nodes). In the middle, the edge statistic is shown for all time points (and for all frequencies
        if applicable). On the sides are shown the spectrograms on each incident node, plotted as the log of the ratio of the power during speech relative to silence.</p>
      <h4 id="credits">Credits</h4>
      <p>The data were provided by <a href="http://www.schalklab.org">Dr. Gerwin Schalk</a> and Dr. Peter Brunner at the Wadsworth Institute in Albany, New York.</p>
      <p>Network analysis was performed by <a href="http://www.emilystephen.com/">Emily Stephen</a> in the <a href="http://www.bu.edu/speechlab/">Speech Lab at Boston University</a>. Details of the analysis may be found in:</p>
      <blockquote>
        <blockquote>
          <p>Stephen, Emily Patricia. 2015. “Characterizing Dynamically Evolving Functional Networks in Humans with Application to Speech.” Order No. 3733680, Boston University. <a href="http://search.proquest.com/docview/1731940762?accountid=9676" class="uri">http://search.proquest.com/docview/1731940762?accountid=9676</a>.</p>
        </blockquote>
      </blockquote>
      <p>The visualization was created by <a href="http://www.ericdeno.com/">Eric Denovellis</a> under the advisement of <a href="http://cns.bu.edu/Profiles/Bullock.html">Daniel H. Bullock</a> at Boston University.</p>
      <p>Code for this visualization is free to use under the GPL-2.0 license. It is available on <a href="https://github.com/edeno/SpectraVis">Github</a>.</p>
    </div>
  </div>
</body>

</html>
